{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Notebooks to Braket Hybrid Jobs with Papermill\n",
    "\n",
    "In this notebook, we submit a Jupyter notebook to Amazon Braket Hybrid Jobs. We download the resultant notebook. \n",
    "\n",
    "## Steps\n",
    "1. First, we add the parameters tag to the top cell of our Jupyter notebook. This keeps all parameters in one place so they are easy to find. In this example, the parameter is `shots`. \n",
    "2. Add the `device_arn` and `results_dir` parameters to the same cell. These are Braket specific parameters that are needed to run the on the currect device with priority and to recover any files saved from the notebook.\n",
    "3. Make sure the any files the notebook saves are prefixed with  `f\"{results_dir}/`. For example, in the notebook `plt.savefig(f\"{fig_dir}/histogram.png\")`\n",
    "4. In a new notebook, set the parameters using a Python dictionary. Do not include the the `device_arn` and `results_dir`. These are braket specifc parameters.\n",
    "5. Create a job using the provded `runner.py` script and pointing to the source notebook. \n",
    "6. Download the results with the helper function `download_result_notebook(dir_name):` where `dir_name` is the name of the directory you want to save results to. \n",
    "7. Open the directory to see the completed notebook and any saved files. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from braket.aws import AwsQuantumJob\n",
    "from braket.jobs.local import LocalQuantumJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to remmeber all the hyperparamters we want to run the notebook with. \n",
    "For reference, we can look at the top cell of `notebook.ipynb` to find all hyperparamters. \n",
    "\n",
    "We specify the hyperparamters as a Python dictionary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"shots\": 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_arn = \"arn:aws:braket:::device/quantum-simulator/amazon/sv1\"\n",
    "\n",
    "job = AwsQuantumJob.create(\n",
    "    device=device_arn,\n",
    "    source_module=\"src\",\n",
    "    entry_point=\"src.runner\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    input_data=\"src/0_Getting_started_papermill.ipynb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wait for the job to complete. When it finishes, the following cell with return `{}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the Jupyter notebook we ran within the job. First we make a directory to save the results to. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from boto3.session import Session\n",
    "\n",
    "def download_result_notebook(dir_name=\"result\"):\n",
    "    PATH = f\"{dir_name}\"\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "\n",
    "    path = job.metadata()[\"outputDataConfig\"][\"s3Path\"] + \"/output/model.tar.gz\"\n",
    "    bucket_name = path.split(\"/\")[2]\n",
    "    output_path = \"/\".join(path.split(\"/\")[3:])\n",
    "\n",
    "    session = Session()\n",
    "    s3 = session.resource(\"s3\")\n",
    "    your_bucket = s3.Bucket(bucket_name)\n",
    "    your_bucket.download_file(output_path, f\"{dir_name}/model.tar.gz\")\n",
    "\n",
    "    with tarfile.open(f\"{dir_name}/model.tar.gz\") as file:\n",
    "        file.extractall(f\"./{dir_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_result_notebook(\"result3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use AWS boto to find the correct directory, then use the boto S3 client to download the results files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there should be a file called `0_Getting_started_papermill.ipynb` in the result directory. This notebook was run with `hyperparameters` within the Braket job. \n",
    "\n",
    "We can also see that the `0_Getting_started_papermill.ipynb` saved the histogram figure as a PNG file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('fresh-braket')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31fa855883ad53c8cdb107e0d9a4d8eeec0772249265172e161f5b93831d35e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
